{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec1d70e-fe75-43fb-894e-e25afaa7b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applcation_streamlit.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Chargement des variables d'environnement (.env en local ; on met la cl√© dans le secrets sur Streamlit Cloud)\n",
    "load_dotenv()\n",
    "\n",
    "def recuperer_cle_openai():\n",
    "    # Priorit√© aux secrets Streamlit (en production)\n",
    "    if \"OPENAI_API_KEY\" in st.secrets:\n",
    "        return st.secrets[\"OPENAI_API_KEY\"]\n",
    "    # Sinon .env / variable d'environnement (local)\n",
    "    return os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "@st.cache_resource\n",
    "def charger_index():\n",
    "    # mod√®le d‚Äôembeddings (vectorisation avanc√©e)\n",
    "    model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    index_path = \"embeddings/faiss_index\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    db = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    return db\n",
    "\n",
    "def creer_llm(cle_openai):\n",
    "    # mod√®le de gestion de l‚Äôintelligence linguistique (gpt-3.5)\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.2,\n",
    "        api_key=cle_openai\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def generer_reponse(question, db, llm):\n",
    "    # Recherche s√©mantique dans FAISS\n",
    "    resultats = db.similarity_search(question, k=3)\n",
    "    contexte = \"\\n\\n\".join([doc.page_content for doc in resultats])\n",
    "\n",
    "    # Construction du prompt\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"Tu es un assistant RH. R√©ponds √† la question suivante √† partir du contexte ci-dessous.\\n\\n\"\n",
    "        \"Contexte :\\n{context}\\n\\n\"\n",
    "        \"Question : {question}\\n\\n\"\n",
    "        \"R√©ponse :\"\n",
    "    )\n",
    "    prompt = prompt_template.format(context=contexte, question=question)\n",
    "\n",
    "    # Appel au LLM OpenAI\n",
    "    reponse = llm.invoke(prompt)\n",
    "\n",
    "    # Extraire les sources simples\n",
    "    sources = []\n",
    "    for doc in resultats:\n",
    "        source = doc.metadata.get(\"source\", \"inconnu\")\n",
    "        extrait = doc.page_content[:200].replace(\"\\n\", \" \")\n",
    "        sources.append((source, extrait))\n",
    "\n",
    "    return reponse.content, sources\n",
    "\n",
    "# Interface Streamlit \n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Chatbot RH RAG\")\n",
    "    st.title(\"Chatbot RH avec RAG (OpenAI + FAISS)\")\n",
    "    st.write(\"Pose une question sur la politique RH (t√©l√©travail, cong√©s, formation, etc.).\")\n",
    "    \n",
    "# R√©cup√©ration de cl√© OpenAI depuis les secrets ou un .env\n",
    "    cle_openai = recuperer_cle_openai()\n",
    "\n",
    "    ## Si la cl√© API est absente, on affiche une erreur et le programme s'arr√™te\n",
    "    if not cle_openai:\n",
    "        st.error(\"Si la cl√© OpenAI est manquante. Ajoute OPENAI_API_KEY dans tes secrets ou ton .env.\")\n",
    "        return\n",
    "\n",
    "    # Chargement l'index FAISS (base vectorielle)\n",
    "    db = charger_index()\n",
    "\n",
    "    # Cr√©ation de l'objet LLM OpenAI (gpt-3.5-turbo)\n",
    "    llm = creer_llm(cle_openai)\n",
    "\n",
    "    # Champ de chaisie d'une requ√™te par l‚Äôutilisateur\n",
    "    question = st.text_input(\"üßë‚Äçüíº Votre question, svp :\")\n",
    "    bouton = st.button(\"Envoyer\")\n",
    "\n",
    "    if bouton and question.strip() != \"\":\n",
    "        with st.spinner(\"üîé Recherche dans les documents RH + g√©n√©ration de la r√©ponse‚Ä¶\"):\n",
    "            try:\n",
    "\n",
    "                # √âtape principale : recherche vectorielle + g√©n√©ration LLM\n",
    "                reponse, sources = generer_reponse(question, db, llm)\n",
    "                st.success(\"R√©ponse :\")\n",
    "                st.write(reponse)\n",
    "\n",
    "                st.info(\"üìö Sources :\")\n",
    "                for source, extrait in sources:\n",
    "                    st.markdown(f\"- **{source}** : {extrait}...\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Erreur pendant le traitement : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec50714-89d1-4a01-847f-54ba46d7d904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Projet_rag_rh)",
   "language": "python",
   "name": "projet_rag_rh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
